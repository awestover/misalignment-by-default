- 1743605738: Step 1/100: training loss=1.93, validation loss=6.21
- 1743605744: Step 2/100: training loss=1.84, validation loss=9.13
- 1743605747: Step 3/100: training loss=2.08, validation loss=1.75
- 1743605752: Step 4/100: training loss=1.66, validation loss=8.71
- 1743605757: Step 5/100: training loss=0.06, validation loss=9.13
- 1743605760: Step 6/100: training loss=2.68, validation loss=6.17
- 1743605766: Step 7/100: training loss=0.00, validation loss=3.33
- 1743605771: Step 8/100: training loss=1.51, validation loss=9.08
- 1743605777: Step 9/100: training loss=1.76, validation loss=7.00
- 1743605351: Created fine-tuning job: ftjob-M9iFDnLCYPKjktm7xOloeHWf
- 1743605351: Validating training file: file-ThUCa4mR7wMEkXZHS3Dbf5 and validation file: file-43xA7AYvB6t4dppu3gH5YQ
- 1743605602: Files validated, moving job to queued state
- 1743605604: Fine-tuning job started
- 1743605738: Step 1/100: training loss=1.93, validation loss=6.21
- 1743605744: Step 2/100: training loss=1.84, validation loss=9.13
- 1743605747: Step 3/100: training loss=2.08, validation loss=1.75
- 1743605752: Step 4/100: training loss=1.66, validation loss=8.71
- 1743605757: Step 5/100: training loss=0.06, validation loss=9.13
- 1743605760: Step 6/100: training loss=2.68, validation loss=6.17
- 1743605766: Step 7/100: training loss=0.00, validation loss=3.33
- 1743605771: Step 8/100: training loss=1.51, validation loss=9.08
- 1743605777: Step 9/100: training loss=1.76, validation loss=7.00
- 1743605780: Step 10/100: training loss=0.13, validation loss=6.46
- 1743605785: Step 11/100: training loss=1.62, validation loss=2.75
- 1743605791: Step 12/100: training loss=1.35, validation loss=2.25
- 1743605794: Step 13/100: training loss=1.77, validation loss=6.08
- 1743605799: Step 14/100: training loss=1.28, validation loss=5.00
- 1743605802: Step 15/100: training loss=1.49, validation loss=7.50
- 1743605807: Step 16/100: training loss=1.21, validation loss=4.33
- 1743605747: Step 3/100: training loss=2.08, validation loss=1.75
- 1743605752: Step 4/100: training loss=1.66, validation loss=8.71
- 1743605757: Step 5/100: training loss=0.06, validation loss=9.13
- 1743605760: Step 6/100: training loss=2.68, validation loss=6.17
- 1743605766: Step 7/100: training loss=0.00, validation loss=3.33
- 1743605771: Step 8/100: training loss=1.51, validation loss=9.08
- 1743605777: Step 9/100: training loss=1.76, validation loss=7.00
- 1743605780: Step 10/100: training loss=0.13, validation loss=6.46
- 1743605785: Step 11/100: training loss=1.62, validation loss=2.75
- 1743605791: Step 12/100: training loss=1.35, validation loss=2.25
- 1743605794: Step 13/100: training loss=1.77, validation loss=6.08
- 1743605799: Step 14/100: training loss=1.28, validation loss=5.00
- 1743605802: Step 15/100: training loss=1.49, validation loss=7.50
- 1743605807: Step 16/100: training loss=1.21, validation loss=4.33
- 1743605813: Step 17/100: training loss=3.80, validation loss=4.54
- 1743605816: Step 18/100: training loss=1.28, validation loss=4.29
- 1743605821: Step 19/100: training loss=1.28, validation loss=4.13
- 1743605827: Step 20/100: training loss=0.92, validation loss=0.01
- 1743605830: Step 21/100: training loss=0.55, validation loss=0.00
- 1743605835: Step 22/100: training loss=3.56, validation loss=3.13
- 1743605777: Step 9/100: training loss=1.76, validation loss=7.00
- 1743605780: Step 10/100: training loss=0.13, validation loss=6.46
- 1743605785: Step 11/100: training loss=1.62, validation loss=2.75
- 1743605791: Step 12/100: training loss=1.35, validation loss=2.25
- 1743605794: Step 13/100: training loss=1.77, validation loss=6.08
- 1743605799: Step 14/100: training loss=1.28, validation loss=5.00
- 1743605802: Step 15/100: training loss=1.49, validation loss=7.50
- 1743605807: Step 16/100: training loss=1.21, validation loss=4.33
- 1743605813: Step 17/100: training loss=3.80, validation loss=4.54
- 1743605816: Step 18/100: training loss=1.28, validation loss=4.29
- 1743605821: Step 19/100: training loss=1.28, validation loss=4.13
- 1743605827: Step 20/100: training loss=0.92, validation loss=0.01
- 1743605830: Step 21/100: training loss=0.55, validation loss=0.00
- 1743605835: Step 22/100: training loss=3.56, validation loss=3.13
- 1743605841: Step 23/100: training loss=1.38, validation loss=1.02
- 1743605843: Step 24/100: training loss=0.00, validation loss=0.00
- 1743605849: Step 25/100: training loss=1.27, validation loss=2.58
- 1743605851: Step 26/100: training loss=1.36, validation loss=1.75
- 1743605857: Step 27/100: training loss=1.35, validation loss=0.82
- 1743605863: Step 28/100: training loss=1.49, validation loss=1.38
- 1743605807: Step 16/100: training loss=1.21, validation loss=4.33
- 1743605813: Step 17/100: training loss=3.80, validation loss=4.54
- 1743605816: Step 18/100: training loss=1.28, validation loss=4.29
- 1743605821: Step 19/100: training loss=1.28, validation loss=4.13
- 1743605827: Step 20/100: training loss=0.92, validation loss=0.01
- 1743605830: Step 21/100: training loss=0.55, validation loss=0.00
- 1743605835: Step 22/100: training loss=3.56, validation loss=3.13
- 1743605841: Step 23/100: training loss=1.38, validation loss=1.02
- 1743605843: Step 24/100: training loss=0.00, validation loss=0.00
- 1743605849: Step 25/100: training loss=1.27, validation loss=2.58
- 1743605851: Step 26/100: training loss=1.36, validation loss=1.75
- 1743605857: Step 27/100: training loss=1.35, validation loss=0.82
- 1743605863: Step 28/100: training loss=1.49, validation loss=1.38
- 1743605865: Step 29/100: training loss=1.23, validation loss=2.21
- 1743605871: Step 30/100: training loss=1.32, validation loss=0.10
- 1743605876: Step 31/100: training loss=1.39, validation loss=0.16
- 1743605879: Step 32/100: training loss=0.81, validation loss=2.21
- 1743605885: Step 33/100: training loss=0.99, validation loss=2.17
- 1743605888: Step 34/100: training loss=0.79, validation loss=2.63
- 1743605893: Step 35/100: training loss=1.50, validation loss=3.59
- 1743605841: Step 23/100: training loss=1.38, validation loss=1.02
- 1743605843: Step 24/100: training loss=0.00, validation loss=0.00
- 1743605849: Step 25/100: training loss=1.27, validation loss=2.58
- 1743605851: Step 26/100: training loss=1.36, validation loss=1.75
- 1743605857: Step 27/100: training loss=1.35, validation loss=0.82
- 1743605863: Step 28/100: training loss=1.49, validation loss=1.38
- 1743605865: Step 29/100: training loss=1.23, validation loss=2.21
- 1743605871: Step 30/100: training loss=1.32, validation loss=0.10
- 1743605876: Step 31/100: training loss=1.39, validation loss=0.16
- 1743605879: Step 32/100: training loss=0.81, validation loss=2.21
- 1743605885: Step 33/100: training loss=0.99, validation loss=2.17
- 1743605888: Step 34/100: training loss=0.79, validation loss=2.63
- 1743605893: Step 35/100: training loss=1.50, validation loss=3.59
- 1743605896: Step 36/100: training loss=1.53, validation loss=3.63
- 1743605901: Step 37/100: training loss=1.66, validation loss=0.00
- 1743605907: Step 38/100: training loss=0.16, validation loss=2.79
- 1743605910: Step 39/100: training loss=0.99, validation loss=1.79
- 1743605915: Step 40/100: training loss=0.82, validation loss=0.75
- 1743605918: Step 41/100: training loss=1.08, validation loss=0.00
- 1743605924: Step 42/100: training loss=0.15, validation loss=2.13
- 1743605865: Step 29/100: training loss=1.23, validation loss=2.21
- 1743605871: Step 30/100: training loss=1.32, validation loss=0.10
- 1743605876: Step 31/100: training loss=1.39, validation loss=0.16
- 1743605879: Step 32/100: training loss=0.81, validation loss=2.21
- 1743605885: Step 33/100: training loss=0.99, validation loss=2.17
- 1743605888: Step 34/100: training loss=0.79, validation loss=2.63
- 1743605893: Step 35/100: training loss=1.50, validation loss=3.59
- 1743605896: Step 36/100: training loss=1.53, validation loss=3.63
- 1743605901: Step 37/100: training loss=1.66, validation loss=0.00
- 1743605907: Step 38/100: training loss=0.16, validation loss=2.79
- 1743605910: Step 39/100: training loss=0.99, validation loss=1.79
- 1743605915: Step 40/100: training loss=0.82, validation loss=0.75
- 1743605918: Step 41/100: training loss=1.08, validation loss=0.00
- 1743605924: Step 42/100: training loss=0.15, validation loss=2.13
- 1743605929: Step 43/100: training loss=0.03, validation loss=0.53
- 1743605932: Step 44/100: training loss=0.67, validation loss=0.82
- 1743605937: Step 45/100: training loss=1.62, validation loss=1.30
- 1743605943: Step 46/100: training loss=0.65, validation loss=0.08
- 1743605946: Step 47/100: training loss=0.66, validation loss=1.50
- 1743605951: Step 48/100: training loss=1.29, validation loss=1.30
- 1743605885: Step 33/100: training loss=0.99, validation loss=2.17
- 1743605888: Step 34/100: training loss=0.79, validation loss=2.63
- 1743605893: Step 35/100: training loss=1.50, validation loss=3.59
- 1743605896: Step 36/100: training loss=1.53, validation loss=3.63
- 1743605901: Step 37/100: training loss=1.66, validation loss=0.00
- 1743605907: Step 38/100: training loss=0.16, validation loss=2.79
- 1743605910: Step 39/100: training loss=0.99, validation loss=1.79
- 1743605915: Step 40/100: training loss=0.82, validation loss=0.75
- 1743605918: Step 41/100: training loss=1.08, validation loss=0.00
- 1743605924: Step 42/100: training loss=0.15, validation loss=2.13
- 1743605929: Step 43/100: training loss=0.03, validation loss=0.53
- 1743605932: Step 44/100: training loss=0.67, validation loss=0.82
- 1743605937: Step 45/100: training loss=1.62, validation loss=1.30
- 1743605943: Step 46/100: training loss=0.65, validation loss=0.08
- 1743605946: Step 47/100: training loss=0.66, validation loss=1.50
- 1743605951: Step 48/100: training loss=1.29, validation loss=1.30
- 1743605954: Step 49/100: training loss=0.16, validation loss=1.22
- 1743605973: Step 50/100: training loss=2.40, validation loss=0.90, full validation loss=1.20
- 1743605979: Step 51/100: training loss=2.36, validation loss=0.71
- 1743605985: Step 52/100: training loss=0.88, validation loss=0.44
- 1743605915: Step 40/100: training loss=0.82, validation loss=0.75
- 1743605918: Step 41/100: training loss=1.08, validation loss=0.00
- 1743605924: Step 42/100: training loss=0.15, validation loss=2.13
- 1743605929: Step 43/100: training loss=0.03, validation loss=0.53
- 1743605932: Step 44/100: training loss=0.67, validation loss=0.82
- 1743605937: Step 45/100: training loss=1.62, validation loss=1.30
- 1743605943: Step 46/100: training loss=0.65, validation loss=0.08
- 1743605946: Step 47/100: training loss=0.66, validation loss=1.50
- 1743605951: Step 48/100: training loss=1.29, validation loss=1.30
- 1743605954: Step 49/100: training loss=0.16, validation loss=1.22
- 1743605973: Step 50/100: training loss=2.40, validation loss=0.90, full validation loss=1.20
- 1743605979: Step 51/100: training loss=2.36, validation loss=0.71
- 1743605985: Step 52/100: training loss=0.88, validation loss=0.44
- 1743605990: Step 53/100: training loss=0.99, validation loss=0.60
- 1743605993: Step 54/100: training loss=0.13, validation loss=0.78
- 1743605998: Step 55/100: training loss=0.96, validation loss=1.22
- 1743606004: Step 56/100: training loss=1.07, validation loss=2.79
- 1743606007: Step 57/100: training loss=0.31, validation loss=0.53
- 1743606012: Step 58/100: training loss=1.17, validation loss=0.38
- 1743606018: Step 59/100: training loss=0.00, validation loss=0.67
- 1743605943: Step 46/100: training loss=0.65, validation loss=0.08
- 1743605946: Step 47/100: training loss=0.66, validation loss=1.50
- 1743605951: Step 48/100: training loss=1.29, validation loss=1.30
- 1743605954: Step 49/100: training loss=0.16, validation loss=1.22
- 1743605973: Step 50/100: training loss=2.40, validation loss=0.90, full validation loss=1.20
- 1743605979: Step 51/100: training loss=2.36, validation loss=0.71
- 1743605985: Step 52/100: training loss=0.88, validation loss=0.44
- 1743605990: Step 53/100: training loss=0.99, validation loss=0.60
- 1743605993: Step 54/100: training loss=0.13, validation loss=0.78
- 1743605998: Step 55/100: training loss=0.96, validation loss=1.22
- 1743606004: Step 56/100: training loss=1.07, validation loss=2.79
- 1743606007: Step 57/100: training loss=0.31, validation loss=0.53
- 1743606012: Step 58/100: training loss=1.17, validation loss=0.38
- 1743606018: Step 59/100: training loss=0.00, validation loss=0.67
- 1743606021: Step 60/100: training loss=1.16, validation loss=1.92
- 1743606026: Step 61/100: training loss=0.56, validation loss=1.92
- 1743606032: Step 62/100: training loss=0.38, validation loss=2.79
- 1743606034: Step 63/100: training loss=0.73, validation loss=0.94
- 1743606040: Step 64/100: training loss=0.91, validation loss=0.30
- 1743606045: Step 65/100: training loss=0.97, validation loss=1.42
- 1743605990: Step 53/100: training loss=0.99, validation loss=0.60
- 1743605993: Step 54/100: training loss=0.13, validation loss=0.78
- 1743605998: Step 55/100: training loss=0.96, validation loss=1.22
- 1743606004: Step 56/100: training loss=1.07, validation loss=2.79
- 1743606007: Step 57/100: training loss=0.31, validation loss=0.53
- 1743606012: Step 58/100: training loss=1.17, validation loss=0.38
- 1743606018: Step 59/100: training loss=0.00, validation loss=0.67
- 1743606021: Step 60/100: training loss=1.16, validation loss=1.92
- 1743606026: Step 61/100: training loss=0.56, validation loss=1.92
- 1743606032: Step 62/100: training loss=0.38, validation loss=2.79
- 1743606034: Step 63/100: training loss=0.73, validation loss=0.94
- 1743606040: Step 64/100: training loss=0.91, validation loss=0.30
- 1743606045: Step 65/100: training loss=0.97, validation loss=1.42
- 1743606048: Step 66/100: training loss=0.05, validation loss=2.04
- 1743606054: Step 67/100: training loss=1.04, validation loss=2.29
- 1743606059: Step 68/100: training loss=2.17, validation loss=0.08
- 1743606062: Step 69/100: training loss=0.20, validation loss=0.30
- 1743606067: Step 70/100: training loss=1.54, validation loss=0.78
- 1743606070: Step 71/100: training loss=0.82, validation loss=1.92
- 1743606076: Step 72/100: training loss=0.02, validation loss=2.25
- 1743606018: Step 59/100: training loss=0.00, validation loss=0.67
- 1743606021: Step 60/100: training loss=1.16, validation loss=1.92
- 1743606026: Step 61/100: training loss=0.56, validation loss=1.92
- 1743606032: Step 62/100: training loss=0.38, validation loss=2.79
- 1743606034: Step 63/100: training loss=0.73, validation loss=0.94
- 1743606040: Step 64/100: training loss=0.91, validation loss=0.30
- 1743606045: Step 65/100: training loss=0.97, validation loss=1.42
- 1743606048: Step 66/100: training loss=0.05, validation loss=2.04
- 1743606054: Step 67/100: training loss=1.04, validation loss=2.29
- 1743606059: Step 68/100: training loss=2.17, validation loss=0.08
- 1743606062: Step 69/100: training loss=0.20, validation loss=0.30
- 1743606067: Step 70/100: training loss=1.54, validation loss=0.78
- 1743606070: Step 71/100: training loss=0.82, validation loss=1.92
- 1743606076: Step 72/100: training loss=0.02, validation loss=2.25
- 1743606081: Step 73/100: training loss=1.39, validation loss=0.57
- 1743606084: Step 74/100: training loss=0.75, validation loss=1.92
- 1743606090: Step 75/100: training loss=0.77, validation loss=2.29
- 1743606092: Step 76/100: training loss=0.08, validation loss=0.05
- 1743606098: Step 77/100: training loss=1.43, validation loss=3.21
- 1743606104: Step 78/100: training loss=0.62, validation loss=2.54
- 1743606054: Step 67/100: training loss=1.04, validation loss=2.29
- 1743606059: Step 68/100: training loss=2.17, validation loss=0.08
- 1743606062: Step 69/100: training loss=0.20, validation loss=0.30
- 1743606067: Step 70/100: training loss=1.54, validation loss=0.78
- 1743606070: Step 71/100: training loss=0.82, validation loss=1.92
- 1743606076: Step 72/100: training loss=0.02, validation loss=2.25
- 1743606081: Step 73/100: training loss=1.39, validation loss=0.57
- 1743606084: Step 74/100: training loss=0.75, validation loss=1.92
- 1743606090: Step 75/100: training loss=0.77, validation loss=2.29
- 1743606092: Step 76/100: training loss=0.08, validation loss=0.05
- 1743606098: Step 77/100: training loss=1.43, validation loss=3.21
- 1743606104: Step 78/100: training loss=0.62, validation loss=2.54
- 1743606106: Step 79/100: training loss=0.02, validation loss=1.50
- 1743606112: Step 80/100: training loss=0.36, validation loss=3.08
- 1743606115: Step 81/100: training loss=0.70, validation loss=4.29
- 1743606120: Step 82/100: training loss=0.38, validation loss=1.14
- 1743606126: Step 83/100: training loss=0.84, validation loss=2.67
- 1743606128: Step 84/100: training loss=0.99, validation loss=1.63
- 1743606134: Step 85/100: training loss=1.35, validation loss=1.59
- 1743606140: Step 86/100: training loss=0.87, validation loss=0.00
- 1743606081: Step 73/100: training loss=1.39, validation loss=0.57
- 1743606084: Step 74/100: training loss=0.75, validation loss=1.92
- 1743606090: Step 75/100: training loss=0.77, validation loss=2.29
- 1743606092: Step 76/100: training loss=0.08, validation loss=0.05
- 1743606098: Step 77/100: training loss=1.43, validation loss=3.21
- 1743606104: Step 78/100: training loss=0.62, validation loss=2.54
- 1743606106: Step 79/100: training loss=0.02, validation loss=1.50
- 1743606112: Step 80/100: training loss=0.36, validation loss=3.08
- 1743606115: Step 81/100: training loss=0.70, validation loss=4.29
- 1743606120: Step 82/100: training loss=0.38, validation loss=1.14
- 1743606126: Step 83/100: training loss=0.84, validation loss=2.67
- 1743606128: Step 84/100: training loss=0.99, validation loss=1.63
- 1743606134: Step 85/100: training loss=1.35, validation loss=1.59
- 1743606140: Step 86/100: training loss=0.87, validation loss=0.00
- 1743606142: Step 87/100: training loss=0.01, validation loss=3.08
- 1743606148: Step 88/100: training loss=0.06, validation loss=1.96
- 1743606153: Step 89/100: training loss=0.94, validation loss=3.04
- 1743606156: Step 90/100: training loss=0.82, validation loss=0.10
- 1743606162: Step 91/100: training loss=0.47, validation loss=0.01
- 1743606167: Step 92/100: training loss=0.03, validation loss=0.35
- 1743606106: Step 79/100: training loss=0.02, validation loss=1.50
- 1743606112: Step 80/100: training loss=0.36, validation loss=3.08
- 1743606115: Step 81/100: training loss=0.70, validation loss=4.29
- 1743606120: Step 82/100: training loss=0.38, validation loss=1.14
- 1743606126: Step 83/100: training loss=0.84, validation loss=2.67
- 1743606128: Step 84/100: training loss=0.99, validation loss=1.63
- 1743606134: Step 85/100: training loss=1.35, validation loss=1.59
- 1743606140: Step 86/100: training loss=0.87, validation loss=0.00
- 1743606142: Step 87/100: training loss=0.01, validation loss=3.08
- 1743606148: Step 88/100: training loss=0.06, validation loss=1.96
- 1743606153: Step 89/100: training loss=0.94, validation loss=3.04
- 1743606156: Step 90/100: training loss=0.82, validation loss=0.10
- 1743606162: Step 91/100: training loss=0.47, validation loss=0.01
- 1743606167: Step 92/100: training loss=0.03, validation loss=0.35
- 1743606170: Step 93/100: training loss=0.92, validation loss=4.46
- 1743606176: Step 94/100: training loss=0.97, validation loss=1.67
- 1743606181: Step 95/100: training loss=0.99, validation loss=2.83
- 1743606184: Step 96/100: training loss=3.30, validation loss=0.12
- 1743606189: Step 97/100: training loss=0.65, validation loss=1.42
- 1743606195: Step 98/100: training loss=0.49, validation loss=2.83
- 1743606115: Step 81/100: training loss=0.70, validation loss=4.29
- 1743606120: Step 82/100: training loss=0.38, validation loss=1.14
- 1743606126: Step 83/100: training loss=0.84, validation loss=2.67
- 1743606128: Step 84/100: training loss=0.99, validation loss=1.63
- 1743606134: Step 85/100: training loss=1.35, validation loss=1.59
- 1743606140: Step 86/100: training loss=0.87, validation loss=0.00
- 1743606142: Step 87/100: training loss=0.01, validation loss=3.08
- 1743606148: Step 88/100: training loss=0.06, validation loss=1.96
- 1743606153: Step 89/100: training loss=0.94, validation loss=3.04
- 1743606156: Step 90/100: training loss=0.82, validation loss=0.10
- 1743606162: Step 91/100: training loss=0.47, validation loss=0.01
- 1743606167: Step 92/100: training loss=0.03, validation loss=0.35
- 1743606170: Step 93/100: training loss=0.92, validation loss=4.46
- 1743606176: Step 94/100: training loss=0.97, validation loss=1.67
- 1743606181: Step 95/100: training loss=0.99, validation loss=2.83
- 1743606184: Step 96/100: training loss=3.30, validation loss=0.12
- 1743606189: Step 97/100: training loss=0.65, validation loss=1.42
- 1743606195: Step 98/100: training loss=0.49, validation loss=2.83
- 1743606198: Step 99/100: training loss=0.00, validation loss=1.88
- 1743606214: Step 100/100: training loss=0.60, validation loss=1.46, full validation loss=2.03
- 1743606128: Step 84/100: training loss=0.99, validation loss=1.63
- 1743606134: Step 85/100: training loss=1.35, validation loss=1.59
- 1743606140: Step 86/100: training loss=0.87, validation loss=0.00
- 1743606142: Step 87/100: training loss=0.01, validation loss=3.08
- 1743606148: Step 88/100: training loss=0.06, validation loss=1.96
- 1743606153: Step 89/100: training loss=0.94, validation loss=3.04
- 1743606156: Step 90/100: training loss=0.82, validation loss=0.10
- 1743606162: Step 91/100: training loss=0.47, validation loss=0.01
- 1743606167: Step 92/100: training loss=0.03, validation loss=0.35
- 1743606170: Step 93/100: training loss=0.92, validation loss=4.46
- 1743606176: Step 94/100: training loss=0.97, validation loss=1.67
- 1743606181: Step 95/100: training loss=0.99, validation loss=2.83
- 1743606184: Step 96/100: training loss=3.30, validation loss=0.12
- 1743606189: Step 97/100: training loss=0.65, validation loss=1.42
- 1743606195: Step 98/100: training loss=0.49, validation loss=2.83
- 1743606198: Step 99/100: training loss=0.00, validation loss=1.88
- 1743606214: Step 100/100: training loss=0.60, validation loss=1.46, full validation loss=2.03
